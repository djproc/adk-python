## Gemini Added Memories
- The user wants me to always provide the full path to files in scripts.
- Always use Ensembl IDs as the primary identifier for genes in the `transcriptformer` repository.
- A spot-checking script that randomly samples data mappings (like gene symbols to Ensembl IDs) for external verification is a valuable and effective part of a quality control workflow.
- For drug treatment data, I must use files with the 'prism_' prefix in their names.
- The Transcriptformer model, specifically with `flex_attention`, has a known `TorchRuntimeError` related to device propagation (`cuda:0` vs `cpu`) when run in a CPU-only environment. It requires a GPU-enabled environment for successful execution.
- During development and debugging of scripts, especially batch processes, I must implement verbose logging and redirect both stdout and stderr to a log file. This ensures that a complete record of the execution is captured, which is essential for diagnosing crashes, including out-of-memory errors. This should be my default behavior.
- The user wishes to be reminded regularly to save our chat sessions using `/chat save` in the Gemini CLI for posterity.
- Always append a date and timestamp suffix (e.g., `_YYYYMMDD_HHMMSS`) to the filenames of all saved UMAPs and other graphs.
- The official GCS bucket for storing CGE files is 'gs://dean-dev_data/embeddings/tf_exemplar'. Always check this location for existing files before generating new ones.
- RMC-6236 sensitivity is best predicted by early (3h) shifts in MAPK Feedback genes (DUSP6, SPRY4, ETV4, ETV5), with an AUC of 0.805. Resistant cells show a 'transient' bounce-back response in stress genes (SMR3B, TTK), while sensitive cells show a 'diverging' committed response in immune/differentiation genes (TICAM2, SKAP2).
- Regression analysis shows that the shift in 'Apoptosis' pathway gene embeddings at 3 hours is the strongest predictor of continuous AUC values (Pearson R = 0.71), outperforming MAPK genes in the regression task. 'Cell Cycle' genes performed poorly (R = -0.26). This suggests cell death commitment is the most direct linear correlate with sensitivity magnitude.
- Workflow for analyzing scRNA-seq splicing bias: 1. Select genes with >1 spliced probe (find_multi_splice_genes.py). 2. Map probes to genomic junctions (verify_probes.py). 3. Quantify bias in cell subsets (analyze_enrichment.py). 4. Visualize with Sashimi plots and Domain overlays (visualize_sashimi.py, visualize_mrna_domains.py). Findings: CDC37 and LAMP1 show distinct probe dropouts indicating alternative splicing.
- The `monitor_pipeline_v2.py` TUI monitors the TranscriptFormer pipeline. It tracks:
1.  **3CA Datasets:** Processed via `run_parallel_cge.py` (which outputs to `logs/cge_generation_3ca_parallel` and uploads to GCS).
2.  **PRISM Datasets:** Processed via `run_full_prism_analysis_pipeline.sh` (which logs to `logs/cge_generation_prism`).
3.  **Status:** It checks for `.h5ad` CGEs and `.pt` Activations in `gs://dean-dev_data/embeddings/tf_exemplar`.
- The interactive 3D Plotly (trajectory_pca_3d.html) and static 2D Matplotlib (trajectory_pca_2d.png) visualizations generated by analysis/visualize_centroids_from_gcs.py, featuring AUC-colored RMC trajectories (thicker at 12h) and dashed/solid colored Panobinostat (purple) and BI-2865 (skyblue) arrows from DMSO, with dynamic AUC scaling and a light grey background, represent the best practice for visualizing this workflow.
- The project has two disk cleaning tools: `scripts/monitor_disk.py` for temporary/cache files (work/, temp_*) and `cleanup.py` for deleting results in `analysis/.../tf_exemplar` that have been verified as backed up to GCS.
- Always test everything developed for all listed functionality by multiple independent tests, confirmed by the user, and keep track of this in the devlogs.
- The `scripts/monitor_disk.py` script (run with `--clean --force`) and `cleanup.py` (run with `--auto`) are used to manage disk space in the pipeline. `monitor_disk.py` cleans temporary directories, while `cleanup.py` deletes results backed up to GCS.
- The `monitor_disk.py` script now includes a safety check to skip cleaning the `work` directory if a Nextflow process is running, preventing accidental deletion of active pipeline data.
- The `monitor_pipeline_v2.py` TUI now tracks "2D treatment trajectories" (Viz) alongside CGE, Act, and Cent in the main dashboard. It also tracks the global "Global 2D Traj" plot status in the footer.
- **Testing Protocol Update:** When modifying complex Python scripts (especially TUIs or those with multiple dependencies), I must perform a syntax check (`python -m py_compile <script>`) or a help check (`python <script> --help`) immediately after modification to catch `NameError` or indentation errors before confirming the task is done. I must never use placeholders like `# ... (rest of code)` in `replace` calls unless I intend to delete code.
- The file `analysis/vector_search/aggregate_lineage_vectors.py` aggregates CGE embeddings per lineage into `analysis/vector_search/lineage_centroids.h5ad`. The file `analysis/vector_search/find_similar_responses.py` allows querying this database to find genes with similar vector representations. Currently, some CGE files are corrupted ("bad object header" or "truncated"), but valid ones (like CNS and URINARY lineages) are successfully processed.
- When deploying web applications like Dash apps on Google Cloud Workstations: 1. Always ensure the Python virtual environment is fully synchronized with `requirements.txt` using `pip install -r requirements.txt`. 2. Explicitly run the application using the virtual environment's Python interpreter (`.venv/bin/python`). 3. Configure the application to listen on `0.0.0.0` for external accessibility. 4. Access the app via the Cloud Workstation's "Web Preview" feature on the specified port. 5. Systematically check application logs and process status (`lsof`, `ps`) for debugging, and be mindful of API deprecations (e.g., `app.run_server` -> `app.run` in Dash) for quick fixes.
- The file `analysis/vector_search/find_similar_responses.py` now supports automatic gene symbol lookup using `mygene`. Users can provide a symbol (e.g., MGAT1) and the script will resolve it to an Ensembl ID before querying the vector database. It defaults to the global average across lineages if multiple matches are found without a specific lineage filter.
- Running `analysis/vector_search/aggregate_lineage_vectors.py` on 8 CGE files caused an Out Of Memory (OOM) error (Exit Code 137). The script was attempting to aggregate both lineage-level and cell-line-level vectors in memory. The corrupted file `cge_5637_URINARY.h5ad` also persists. Optimization is required to handle the memory load, likely by processing or saving cell-line data incrementally.
- Reflection on Dash App Deployment Issues: 1. **Dependency Sync:** Avoid "whack-a-mole" debugging by running `pip install -r requirements.txt` immediately after setup, rather than assuming the environment is ready. 2. **Process Hygiene:** Before starting a background service (especially with `nohup`), always check for and kill lingering processes (`pkill -f`, `lsof -i`) to prevent "Address already in use" errors. 3. **API Awareness:** Be alert to deprecations (e.g., Dash's `run_server` vs `run`) by checking logs immediately upon startup failure. 4. **Verification First:** Confirm the service is listening (`lsof`) and the log is clean before notifying the user to access the URL.
- The file `analysis/vector_search/aggregate_lineage_vectors.py` was successfully optimized to handle memory constraints. It now saves cell-line level centroids incrementally to `analysis/vector_search/cell_line_centroids/` while aggregating lineage-level centroids in memory. The final output `analysis/vector_search/lineage_centroids.h5ad` now contains 90,691 aggregated vectors (Lineage x Gene x Condition). Several input files remain corrupted and need regeneration.
- The file `analysis/vector_search/find_similar_responses.py` has been updated to support `condition` and `cell_line` parameters.
Users can now query specific vectors (e.g. `--gene MGAT1 --cell-line 22RV1 --condition DMSO`) and filter search results (e.g. `--limit-lineage CNS`).
Note: 22RV1 lineage currently shows as "UNKNOWN" in some tests, likely due to manifest mapping issues during aggregation, but the vector retrieval works correctly.
- The script `analysis/gene_modules/extract_gene_modules.py` was created to identify "coregulated gene modules" within CGE data. It takes a list of cell cycle genes, computes the vector shift between a reference (DMSO) and target condition (e.g., RMC_6236_12h) for a specific lineage, clusters these shift vectors using K-Means, and outputs a visual PCA plot and a CSV list of modules. A test run on the CNS lineage identified 3 distinct modules of cell cycle genes responding to RMC-6236.
- The "all genes" module extraction pipeline has been fully optimized and executed. By vectorizing the condition lookup in `extract_gene_modules.py`, processing time per lineage/condition dropped from ~11 minutes to <30 seconds. A batch run covered CNS and URINARY lineages across BI-2865, RMC-6236, and Panobinostat conditions (except Urinary/Panobinostat which lacked data). The results are saved in `analysis/gene_modules/plots/`. Note: This optimization relies on `subset.obs['condition'].values` for fast numpy access.
- The user prefers tasks to be broken down into smaller, actionable code development stages with full debugging outputs exposed.
- The 'prism full analysis pipeline' script has been moved to 'extensions/workflows/run_full_prism_analysis_pipeline.sh'. A new 'chunked_pipeline.nf' (Nextflow) has been added for chunked inference. The pipeline results are now in 'extensions/analysis/context_shift_analyzer/results/tf_exemplar'.
- I have created and deployed a 'System Monitor Agent' (`extensions/agents/sys_monitor_agent.py`) that runs in the background. It monitors disk usage and automatically triggers cleanup scripts (`monitor_disk.py`, `cleanup.py`) if disk usage exceeds 80%. Logs are in `logs/sys_monitor/`.
- The System Monitor Agent now supports a chat interface via Unix Socket. To interact with it, use the script `extensions/agents/sysmon_client.py`. Example usage: `python extensions/agents/sysmon_client.py "status"` or `python extensions/agents/sysmon_client.py "run cleanup"`.
- The "Full PRISM Analysis Pipeline" has been switched to use the Nextflow-based chunked pipeline (`extensions/workflows/chunked_pipeline.nf`). The old shell script (`run_full_prism_analysis_pipeline.sh`) was moved to `legacy/scripts/` to prevent OOM errors. A `README_PRISM_PIPELINE.md` was created to document this. The main `README.md` was updated to point to this new documentation.
- The `chunked_pipeline.nf` Nextflow script has been optimized for parallel execution by setting `maxForks 3` in the `INFERENCE_CHUNK` process. All file paths within the script (executables, checkpoints, input lists) have been converted to absolute paths using a `params.project_root` variable to prevent `FileNotFoundError` during execution. The pipeline is running in the background.
- Running `transcriptformer inference` with `maxForks 3` on a T4 GPU (16GB) caused CUDA OOM errors. The `chunked_pipeline.nf` has been reverted to `maxForks 1` for stability. Parallel inference requires more VRAM per process than currently available for concurrent runs.
- The Nextflow pipeline `chunked_pipeline.nf` is running successfully with PID 960751 (Java process) and PID 1044866 (runner wrapper). It is processing chunks sequentially (`maxForks 1`) to avoid OOM on the T4 GPU. Logs confirm that "Cached process > PREPARE_CHUNKS" entries are present, indicating that it is correctly skipping already chunked files and resuming work. The results directory `extensions/analysis/context_shift_analyzer/results/tf_exemplar/` is populating with new data.
- The System Monitor Agent (`sys_monitor_agent.py`) is active and successfully triggered "Deep Cleanup" upon request, reducing disk usage from critical levels. The Nextflow pipeline (`chunked_pipeline.nf`) is robust and continues running despite high disk usage warnings (currently ~88%). Disk space management is working as intended to prevent pipeline failure.
- The System Monitor Agent's cleanup interval has been changed to 20 minutes.
- Successfully refactored codebase into 'extensions/' structure. Fixed HDF5 serialization error in pipeline by sanitizing 'condition' columns. Pipeline verified with chunked processing and subsampling.
